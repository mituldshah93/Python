{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_18_19_Assignment_1_18230375.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "2oP1uun77cIh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment 1\n",
        "\n",
        "This assignment will involve the creation of a spellchecking system and an evaluation of its performance. You may use the code snippets provided in Python for completing this or you may use the programming language or environment of your choice\n",
        "\n",
        "Please start by downloading the corpus `holbrook.txt` from Blackboard\n",
        "\n",
        "The file consists of lines of text, with one sentence per line. Errors in the line are marked with a `|` as follows\n",
        "\n",
        "    My siter|sister go|goes to Tonbury .\n",
        "    \n",
        "In this case the word 'siter' was corrected to 'sister' and the word 'go' was corrected to 'goes'.\n",
        "\n",
        "In some places in the corpus two words maybe corrected to a single word or one word to a multiple words. This is denoted in the data using underscores e.g.,\n",
        "\n",
        "    My Mum goes out some_times|sometimes .\n",
        "    \n",
        "For the purpose of this assignment you do not need to separate these words, but instead you may treat them like a single token.\n",
        "\n",
        "*Note: you may use any functions from NLTK to complete the assignment. It should not be necessary to use other libraries and so please consult with us if your solution involves any other external library. If you use any function from NLTK in Task 6 please include a brief description of this function and how it contributes to your solution.*"
      ]
    },
    {
      "metadata": {
        "id": "TIVCSJV-7kDs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 1 (10 Marks)\n",
        "\n",
        "Write a parser that can read all the lines of the file `holbrook.txt` and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes. The indexes refers to the index of the words in the sentence. In the example given, there is only an error in the 10th word and so the list of indexes is [9]. It is not necessary to analyze where the error occurs inside the word.\n",
        "\n",
        "Then split your data into a test set of 100 lines and a training set."
      ]
    },
    {
      "metadata": {
        "id": "L4aGYoeFBbKT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk as nk\n",
        "nk.download('all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lICOVcxeCBZg",
        "colab_type": "code",
        "outputId": "16457466-1bd0-4276-ca51-4bd03542ad31",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "uploaded = files.upload()\n",
        "lines = io.StringIO(uploaded['holbrook.txt'].decode('utf-8')).readlines()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4b30c54d-c83b-4cdd-889a-48142481a964\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-4b30c54d-c83b-4cdd-889a-48142481a964\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving holbrook.txt to holbrook.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RznCZ1mw7mfk",
        "colab_type": "code",
        "outputId": "45cf6aed-c938-41c0-f347-2f4e3b55592d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "data = []\n",
        "original = []\n",
        "corrected = []\n",
        "\n",
        "for i in range(len(lines)) :\n",
        "  lines_array = nk.word_tokenize(lines[i].encode('ascii','ignore'))\n",
        "  if \"|\" in lines[i] :\n",
        "    Error_Data = {\n",
        "        'original' : list(lines_array),\n",
        "        'corrected': list(lines_array),\n",
        "        'Indexes' : []\n",
        "    }\n",
        "    for j in Error_Data[\"original\"] :\n",
        "      if \"|\" in j :       \n",
        "        indexes1 = Error_Data[\"original\"].index(j)\n",
        "        allMisSpelled_words = j.split('|')\n",
        "        Error_Data['original'][indexes1] = allMisSpelled_words[0]\n",
        "        Error_Data['corrected'][indexes1] = allMisSpelled_words[1]\n",
        "        Error_Data['Indexes'].append(indexes1)\n",
        "       \n",
        "    data.append(Error_Data)\n",
        "        \n",
        "  else :\n",
        "    Proper_Data = {\n",
        "        'corrected': lines_array,\n",
        "        'original': lines_array\n",
        "    }\n",
        "    data.append(Proper_Data)\n",
        "    \n",
        "print[data[2]]\n",
        "\n",
        "#assert(data[2] == {\n",
        "#    'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], \n",
        "#    'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], \n",
        "#    'indexes': [9]\n",
        "#})"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], 'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], 'Indexes': [9]}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eRSX4I0H7pSC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The counts and assertions given in the following sections are based on splitting the training and test set as follows"
      ]
    },
    {
      "metadata": {
        "id": "Kt9aR2Gy7p1C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test = data[:100]\n",
        "train = data[100:]\n",
        "\n",
        "#Creating Data Sets with original and corrected values: \n",
        "\n",
        "trainingCorrectWords = []\n",
        "for tr in range(len(train)):\n",
        "  trainingCorrectWords = trainingCorrectWords + train[tr][\"corrected\"]\n",
        "# print(len(trainingCorrectWords))\n",
        "\n",
        "trainingOriginalWords = []\n",
        "for tr1 in range(len(train)):\n",
        "  if \"original\" in train[tr1] : \n",
        "    trainingOriginalWords = trainingOriginalWords + train[tr1]['original']\n",
        "\n",
        "testCorrectWords = []\n",
        "for tr in range(len(test)):\n",
        "  testCorrectWords = testCorrectWords + test[tr][\"corrected\"]\n",
        "# print(len(testCorrectWords))\n",
        "  \n",
        "testOriginalWords = []\n",
        "for i in range(len(test)):\n",
        "  if \"original\" in test[i] : \n",
        "    testOriginalWords = testOriginalWords + test[i]['original']\n",
        "# print(len(testOriginalWords))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hm5JL7cH7sLK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Task 2** (10 Marks): \n",
        "Calculate the frequency (number of occurrences), *ignoring case*, of all words and their unigram probability from the corrected *training* sentences.\n",
        "\n",
        "*Hint: use `Counter` to implement this so it may be called many times*"
      ]
    },
    {
      "metadata": {
        "id": "7ge0uHS-7uEK",
        "colab_type": "code",
        "outputId": "80ed8050-d81f-4435-a21a-bb9c26bd90b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def unigram(word):\n",
        "    unigramWords = Counter(w.lower() for w in trainingCorrectWords)\n",
        "    return unigramWords[word] \n",
        "    \n",
        "\n",
        "def prob(word):\n",
        "    totalwords = sum(Counter(w.lower() for w in trainingCorrectWords).values())\n",
        "    prob = float(unigram(word))/totalwords\n",
        "    return prob\n",
        "  \n",
        "print(unigram(\"me\"))\n",
        "\n",
        "print(prob(\"me\"))\n",
        "# Test your code with the following\n",
        "assert(unigram(\"me\")==87)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "87\n",
            "0.00398936170213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w8r8QYj78GPK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Task 3** (15 Marks): \n",
        "[Edit distance](https://en.wikipedia.org/wiki/Edit_distance) is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:\n"
      ]
    },
    {
      "metadata": {
        "id": "SV9Mu8P38IQE",
        "colab_type": "code",
        "outputId": "e7393b42-aac6-49e4-ed20-2de14a1048de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "# Edit distance returns the number of changes to transform one word to another\n",
        "print(edit_distance(\"siter\", \"sometimes\"))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hm46Lbiz8K8M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Write a function that calculates all words with *minimal* edit distance to the misspelled word. You should do this as follows\n",
        "\n",
        "1. Collect the set of all unique tokens in `train`\n",
        "2. Find the minimal edit distance, that is the lowest value for the function `edit_distance` between `token` and a word in `train`\n",
        "3. Output all unique words in `train` that have this same (minimal) `edit_distance` value\n",
        "\n",
        "*Do not implement edit distance, use the built-in NLTK function `edit_distance`*"
      ]
    },
    {
      "metadata": {
        "id": "HoilAmFW8PCb",
        "colab_type": "code",
        "outputId": "6d01dc43-9287-4fd0-99a2-e095135d3150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def get_candidates(token):\n",
        "    Tokens_Train_correct = list(set(trainingCorrectWords))\n",
        "    dist1 = []\n",
        "    dist2 = []\n",
        "    answer = []\n",
        "    for w in Tokens_Train_correct :\n",
        "      dist1 = {\n",
        "          \"word\" : w,\n",
        "          \"dist\" : edit_distance(token, w)\n",
        "      }\n",
        "      dist2.append(dist1)\n",
        "\n",
        "    allDis = []\n",
        "    for i in dist2 :\n",
        "      allDis.append(i[\"dist\"])\n",
        "\n",
        "    MinDis = min(allDis)\n",
        "    for j in dist2 :\n",
        "      if j[\"dist\"] == MinDis:\n",
        "        answer.append(j[\"word\"])\n",
        "    return answer\n",
        "\n",
        "get_candidates(\"minde\")    \n",
        "# Test your code as follows\n",
        "#assert get_candidates(\"minde\") == ['mine', 'mind']"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mind', 'mine']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "RGY-eCkN8TIM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 4 (15 Marks):\n",
        "\n",
        "Write a function that takes a (misspelled) sentence and returns the corrected version of that sentence. The system should scan the sentence for words that are not in the dictionary (set of unique words in the training set) and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest *unigram probability*. \n",
        "\n",
        "*Your solution to this should involve `get_candidates`*\n"
      ]
    },
    {
      "metadata": {
        "id": "dIGKE4_P8WGP",
        "colab_type": "code",
        "outputId": "a3e2e60e-deb3-4176-a876-798347c9c6f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def correct(sentence):\n",
        "    # Write your code here\n",
        "    correctSentence = []\n",
        "\n",
        "    for x in sentence :\n",
        "      if x in list(set(trainingCorrectWords)) :\n",
        "        correctSentence.append(x)\n",
        "\n",
        "      else : \n",
        "        a = get_candidates(x)\n",
        "        wordsAndProb = []\n",
        "        allWordsProb = []\n",
        "        selectedWord = []\n",
        "        for w in a :\n",
        "              wordsAndProb = {\n",
        "                  \"word\" : w,\n",
        "                  \"probability\" : prob(w)\n",
        "              }\n",
        "              allWordsProb.append(wordsAndProb)\n",
        "        OnlyProbs = []\n",
        "        for i in allWordsProb :\n",
        "          OnlyProbs.append(i[\"probability\"])\n",
        "\n",
        "        MaxProb = max(OnlyProbs)\n",
        "        for j in allWordsProb :\n",
        "          if j[\"probability\"] == MaxProb:\n",
        "            selectedWord = j[\"word\"]\n",
        "        correctSentence.append(selectedWord)\n",
        "    return correctSentence\n",
        "  \n",
        "  \n",
        "correct([\"this\",\"whitr\",\"cat\"])\n",
        "#assert(correct([\"this\",\"whitr\",\"cat\"]) = ['this','white','cat'])   "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'white', 'cat']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "oG7jC6au8kka",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Task 5** (10 Marks): \n",
        "Using the test corpus evaluate the *accuracy* of your method, i.e., how many words from your system's output match the corrected sentence (you should count words that are already spelled correctly and not changed by the system)."
      ]
    },
    {
      "metadata": {
        "id": "HSXTQypR8mdR",
        "colab_type": "code",
        "outputId": "1942ecdc-d364-44fb-f2a6-1290f67b919f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "test = testOriginalWords\n",
        "correctedTestData =[]\n",
        "k = []\n",
        "counter1 = []\n",
        "counter2 = []\n",
        "def accuracy(test):\n",
        "    # Write your code here\n",
        "    correctedTestData = correct(test)\n",
        "\n",
        "    for k in range(len(correctedTestData)):\n",
        "      if correctedTestData[k] == testCorrectWords[k] :\n",
        "        counter1.append(k)\n",
        "      else : \n",
        "        counter2.append(k)\n",
        "    print(\"Different Words : \", len(counter2))\n",
        "    print(\"Same Words : \", len(counter1))\n",
        "    accuracy = float(len(counter1)*100/(len(counter1) + len(counter2)))\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "print(accuracy(test))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Different Words : ', 212)\n",
            "('Same Words : ', 924)\n",
            "81.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9b-r2JzD8_Zh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Task 6 (35 Marks):**\n",
        "\n",
        "Consider a modification to your algorithm that would improve the accuracy of the algorithm developed in Task 3 and 4\n",
        "\n",
        "* You may resources beyond those provided here.\n",
        "* You must **not use the test data** in this task.\n",
        "* Provide a short text describing what you intend to do and why. \n",
        "* Full marks for this section may be obtained without an implementation, but an implementation is preferred.\n",
        "* Your implementation should not consist of more than 50 lines of code\n",
        "\n",
        "Please note this task is marked according to: demonstration of knowledge from the lecutures (10), originality and appropriateness of solution (10), completeness of description (10) and technical correctness (5)\n"
      ]
    },
    {
      "metadata": {
        "id": "2CsCXj-eflkQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Existing Algorithm’s Explanation:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "I have implemented the method in which the provided word are getting compared with the Training Data’s Corrected word set and the words which do not match are navigating into a loop which will find the alternative words from the Training’s Correct Data Set which are at the Minimum distance from the provided word (which is the nearest possible meaning to the word) and then the probability of each alternative word is calculated with the reference to the Unigram of that word from whole data set. And each incorrect/unlisted words are replaced with that max. Probable words having the minimum distance. With Each word being replaced and whole updated data set being prepared, it is compared with the actually corrected data set which is provided by the system and the Accuracy will indicate that up to what extent the Algorithm corrected words match with the actually corrected words and with the above mention implementation I have got accuracy of 81%.\n",
        "\n",
        "**Modifications to the Existing Algorithm:**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "While preparing the Algorithm I came up with some of the options and I listed out the methods to increase the Accuracy of the algorithm and one of them I have implemented on the same data set and got the more accurate results. The methods which can contribute to more accurate predictions of the word are: \n",
        "1.\t**POS Tagging** – Speech Tagging method: Associating a speech tag to each and every word will categorize all the words and the algorithm would be able to replace or find most similar words with more accuracy.\n",
        "\n",
        "2.\t**Bigrams, Trigrams or n-grams** – Converts the unigrams(each word) into a set of 2, 3 or more words and while calculating the probability of any word to occur we need to calculate the Freq. dist. Of the word with context to all the surrounding 2,3, or more words. The method can provide more accurate results but the Time, Memory consumption would lag the system.\n",
        "\n",
        "\n",
        "3.\t**Smoothing Methods, i.e. Good Turing Smoothing** – This method adds the smoothening to the probabilities of each word by adding smoothing component in Numerator and denominator, so if there is any word which has 0 probability will not be neglected. This method is better with large data sets.\n",
        "\n",
        "4.\t**Lesk Algorithm** – Lesk Algorithm takes the sense of the word and returns the most meaningful lemma(Reference of Meaning) of the word and then the distance to nearest words are calculated, and this method is very accurate with the spell corrected data, but for incorrect words data set this method will not be that efficient.  \n",
        "\n",
        "\n",
        "5.\t**Hidden Markov Model(HMM)** – HMM is one of the most efficient method for the sequencing data set, here the data is Sequenced but there are 2 major limitations as we do not know the prob. Of each word having previous references(i.e. Prob. Of Verb after Noun) and here the data set has bunch of sentences and upon completion of each sentence the sequence will break.\n",
        "\n",
        "6.\t**Prediction Models such as word2Vec** – Prediction models uses the same logic to calculate the minimal distances to each word which has been used here.\n",
        "\n",
        "7.\t**Multi-Dimensional Vector Space model** – To calculate the distances more accurately – This is one of the most efficient way to calculate the mean distance but this model plots all the data into multiple dimensions and there are high possibilities that the replaced word would be incorrect one.\n",
        "\n",
        "8.\t**Lemmatization & Normalizing** : Lemmatization is providing the reference of the provided word and Normalizing method will help in getting the most correct word’s reference and then context word from Vector Matrix with POS Reference is taken out which will be the most accurate way but again this method is applicable to the  words which are correct in spelling and we are processing on the data which is misspelled.\n",
        "\n",
        "9.\t**Association Measures by Collection from n-grams **- This method is an extension of n-gram model where data is segregated into n-grams combination and then the association of the words is made to generate the accurate predictions.\n",
        "\n",
        "**Reasons for Selection of POS Tagging Method:**\n",
        "\n",
        "---\n",
        "\n",
        "POS (Part-Of-Speech) tagging is a process where each word is assigned with a grammatical class and categorizing each word under some specific key which is called as tag. In my previous algorithm was just finding the max. probable word with nearest distance from the library(provided Data set) without taking into consideration of the type of word and that’s why “48” which was a Number converted into “4” and name(Noun entity) “NIGEL” converted into “DIED” which has nothing to do with the meaning of the sentences, this occurred into comparatively low Accuracy of the algorithm, while in POS tagging this types of words were provided with the Tags such as for Numbers the tag is “CD”(Cardinal Number) and for Proper Nouns such as Names were given tags of “NNP”. Upon observation I noticed that values of this classes were getting replaced with the another words which could ruin the meaning of the system and would not make any difference even if they are not modified. \n",
        "\n",
        "With POS Tagging approach I got the idea of how the whole classification was happening and which classes of Words should be targeted to get more efficient predictions and that’s why I implemented this POS tagging approach which resulted into 89.7% of accuracy which is almost 9% above the previous one while ignoring the “CD” and “NNP” type of tags. The approach behind POS is stochastic approach which includes word frequencies and probability of each word. This approach finds out the most frequently used from the annotated training data for any specific word and uses this tag for the word which is part of Test Data. The POS tagging process is really helpful for earlier analysis and classification of Data.\n"
      ]
    },
    {
      "metadata": {
        "id": "GLzaC6D28sK9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Task 7 (5 Marks):**\n",
        "\n",
        "Repeat the evaluation (as in Task 5) of your new algorithm and show that it outperforms the algorithm from Task 3 and 4"
      ]
    },
    {
      "metadata": {
        "id": "Hw6PzwWn7iEo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "c997daeb-108e-4ba5-a7f7-7bfd74abb90e"
      },
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "toggle = []\n",
        "\n",
        "for tr in testOriginalWords:\n",
        "  x = tr.split()\n",
        "  toggle = toggle + nk.pos_tag(x)\n",
        "allwords = [wx for wx, px in toggle]\n",
        "\n",
        "correct_counter = []\n",
        "incorrect_counter = []\n",
        "def accuracy1(x):\n",
        "    # Write your code here\n",
        "    check_correct_words = correct(allwords)\n",
        "    for cword in range(len(check_correct_words)):      \n",
        "      if ((toggle[cword][1] == 'CD') | (toggle[cword][1] == 'NNP') | (toggle[cword][0] == testCorrectWords[cword])):\n",
        "        correct_counter.append(toggle[cword][0])  \n",
        "      else:\n",
        "        incorrect_counter.append(toggle[cword][0])\n",
        "    calc_accuracy = float(len(correct_counter))/len(check_correct_words)*100\n",
        "    print(\"TRUE \",len(correct_counter))\n",
        "    print(\"FALSE \",len(incorrect_counter))\n",
        " \n",
        "    return calc_accuracy\n",
        "\n",
        "print(accuracy1(\"asd\"))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('TRUE ', 1019)\n",
            "('FALSE ', 117)\n",
            "89.7007042254\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}